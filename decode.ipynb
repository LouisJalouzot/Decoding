{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.main import main\n",
    "from src.prepare_latents import compute_chunks\n",
    "from src.utils import device\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import clear_output\n",
    "import torchmetrics.functional as tmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_decoding.decoding.GPT import GPT\n",
    "from semantic_decoding.decoding.LanguageModel import LanguageModel\n",
    "from semantic_decoding.decoding.Decoder import Decoder\n",
    "\n",
    "data_lm = Path(\"data/data_lm\")\n",
    "with open(data_lm / \"perceived\" / \"vocab.json\", \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(data_lm / \"decoder_vocab.json\", \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = data_lm / \"perceived\" / \"model\", vocab=gpt_vocab, device=device)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass=0.9, nuc_ratio=0.1)\n",
    "decoder = Decoder([1, 2, 3], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['i', 'we', 'she', 'he', 'they', 'it'],\n",
       "  array([-1.79175947, -1.79175947, -1.79175947, -1.79175947, -1.79175947,\n",
       "         -1.79175947]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.beam_propose(decoder.beam, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"li2022_EN_SS_trimmed_mean\"]\n",
    "subjects = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"lebel2023\"]\n",
    "subjects = {\"lebel2023\": [\"UTS03\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"datasets\": datasets,\n",
    "    \"subjects\": subjects,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"decoder\": \"brain_decoder\",\n",
    "    \"loss\": \"mixco\",\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"context_length\": 6,\n",
    "    \"lag\": 3,\n",
    "    \"smooth\": 6,\n",
    "    \"stack\": 0,\n",
    "    \"dropout\": 0.7,\n",
    "    \"patience\": 20,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 1,\n",
    "    \"temperature\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = pipeline(\"text-generation\", model=\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = main(return_data=True, caching=False, **config)\n",
    "_, decoder = main(**config)\n",
    "decoder = decoder.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "df = pd.concat([df_train[[\"dataset\", \"run\"]], df_valid[[\"dataset\", \"run\"]], df_test[[\"dataset\", \"run\"]]]).drop_duplicates()\n",
    "for _, row in df.iterrows():\n",
    "    if row.dataset == \"lebel2023\":\n",
    "        textgrid_path = f\"data/lebel2023/derivative/TextGrids/{row.run}.TextGrid\"\n",
    "    chunks.append([row.dataset, row.run, compute_chunks(textgrid_path, 2, 0)])\n",
    "chunks = pd.DataFrame(chunks, columns=[\"dataset\", \"run\", \"text\"])\n",
    "chunks[\"num_tokens\"] = chunks.text.apply(lambda x: len(gpt2.tokenizer.encode(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"text\"]).merge(chunks)\n",
    "df_valid = df_valid.drop(columns=[\"text\"]).merge(chunks)\n",
    "df_test = df_test.drop(columns=[\"text\"]).merge(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = []\n",
    "for text in chunks.text:\n",
    "    for chunk in text:\n",
    "        num_tokens.append(len(gpt2.tokenizer.encode(chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.histogram(num_tokens, nbins=50, marginal=\"box\", title=\"Number of tokens per chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, row = next(iter(df_train.iterrows()))\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "generated_chunks_lengths = []\n",
    "current_crop_length = 0\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(len(row.text))[:10]:\n",
    "    generated_sentences = gpt2(\n",
    "        prompt,\n",
    "        max_new_tokens=8,\n",
    "        num_return_sequences=1000,\n",
    "        pad_token_id=50256,\n",
    "        top_k=0,\n",
    "        top_p=0.6,\n",
    "        temperature=1.8,\n",
    "        repetition_penalty=1.8,\n",
    "    )\n",
    "    generated_sentences = [re.sub(r'\\n+', ' ', s[\"generated_text\"]) for s in generated_sentences]\n",
    "    if i > config[\"context_length\"]:\n",
    "        current_crop_length = generated_chunks_lengths[i - config[\"context_length\"] - 1]\n",
    "    generated_sentences_cropped = [s[current_crop_length:] for s in generated_sentences]\n",
    "    embeddings = model.encode(generated_sentences_cropped, convert_to_numpy=False, convert_to_tensor=True)\n",
    "    best_sentence_index = tmf.pairwise_cosine_similarity(predicted_latents[[i]], embeddings).argmax()\n",
    "    best_sentence = generated_sentences[best_sentence_index]\n",
    "    generated_chunks_lengths.append(len(best_sentence) - len(prompt))\n",
    "    print(\"Generated: \", best_sentence[len(prompt):])\n",
    "    print(\"Correct: \", row.text[i])\n",
    "    print(i, current_crop_length)\n",
    "    prompt = best_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
