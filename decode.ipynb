{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from src.main import main\n",
    "from src.prepare_latents import compute_chunks\n",
    "from src.utils import device, progress\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import clear_output\n",
    "import torchmetrics.functional as tmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"li2022_EN_SS_trimmed_mean\"]\n",
    "subjects = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"lebel2023\"]\n",
    "subjects = {\"lebel2023\": [\"UTS03\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"datasets\": datasets,\n",
    "    \"subjects\": subjects,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"decoder\": \"brain_decoder\",\n",
    "    \"loss\": \"mixco\",\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"context_length\": 6,\n",
    "    \"lag\": 3,\n",
    "    \"smooth\": 6,\n",
    "    \"stack\": 0,\n",
    "    \"dropout\": 0.7,\n",
    "    \"patience\": 20,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 1,\n",
    "    \"temperature\": 0.05,\n",
    "    # \"top_encoding_voxels\": 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = pipeline(\"text-generation\", model=\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = main(\n",
    "    return_data=True, cache=False, wandb_mode=\"disabled\", **config\n",
    ")\n",
    "# _, decoder = main(wandb_mode=\"disabled\", **config)\n",
    "# decoder = decoder.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train[df_train.run == \"wheretheressmoke\"].iloc[0]\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(tuple(df_train.X))\n",
    "Y = np.concatenate(tuple(df_train.Y))\n",
    "model = Ridge().fit(Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(tuple(df_valid.X))\n",
    "Y = np.concatenate(tuple(df_valid.Y))\n",
    "r2 = r2_score(X, model.predict(Y), multioutput=\"raw_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.argsort()[-1000000:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.histogram(r2[r2 > 0], nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = set(df_train.drop_duplicates([\"dataset\", \"run\"]).text.sum())\n",
    "chunks |= set(df_valid.drop_duplicates([\"dataset\", \"run\"]).text.sum())\n",
    "chunks = pd.Series(list(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train.iloc[1]\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_chunks = []\n",
    "with progress:\n",
    "    task = progress.add_task(f\"Decoding {row.run}\", total=len(row.text))\n",
    "    for i in range(len(row.X)):\n",
    "        context_sentence = \" \".join(decoded_chunks[-config[\"context_length\"]:])\n",
    "        continuations = context_sentence + \" \" + chunks\n",
    "        continuations_latents = model.encode(continuations, convert_to_numpy=False, convert_to_tensor=True)\n",
    "        scores = tmf.pairwise_cosine_similarity(predicted_latents[[i]], continuations_latents)[0].cpu()\n",
    "        best_continuation = chunks[scores.argmax().item()]\n",
    "        decoded_chunks.append(best_continuation)\n",
    "        correct_chunks = row.text[max(0, i-config[\"context_length\"]):i+1]\n",
    "        predicted_chunks = decoded_chunks[-config[\"context_length\"]:]\n",
    "        for j, (correct, predicted) in enumerate(zip(correct_chunks, predicted_chunks)):\n",
    "            if correct == predicted:\n",
    "                correct_chunks[j] = f\"\\033[92m{correct}\\033[0m\"\n",
    "                predicted_chunks[j] = f\"\\033[92m{correct}\\033[0m\"\n",
    "        \n",
    "        print(f\"Chunk {i+1}/{len(row.X)}\")\n",
    "        print(\"Correct  :\", \" \\033[91m|\\033[0m \".join(correct_chunks))\n",
    "        print(\"Predicted:\", \" \\033[91m|\\033[0m \".join(predicted_chunks))\n",
    "        progress.update(task, advance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Tang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_decoding.decoding.GPT import GPT\n",
    "from semantic_decoding.decoding.LanguageModel import LanguageModel\n",
    "from semantic_decoding.decoding.Decoder import Decoder, Hypothesis\n",
    "\n",
    "data_lm = Path(\"data/data_lm\")\n",
    "with open(data_lm / \"perceived\" / \"vocab.json\", \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(data_lm / \"decoder_vocab.json\", \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = data_lm / \"perceived\" / \"model\", vocab=gpt_vocab, device=device)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass=0.9, nuc_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_decoder = Decoder(word_times=range(sum(row.num_words)), beam_width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=sum(row.num_words)) as pbar:\n",
    "    for i, num_words in enumerate(row.num_words):\n",
    "        # if i > 0:\n",
    "        #     print(\"\\033[F\\033[F\", end='')\n",
    "        pbar.set_description(f\"Chunk {i+1} / {len(row.num_words)}\")\n",
    "        context_window = sum(row.num_words[max(0, i-config[\"context_length\"]):i])\n",
    "        for _ in range(num_words):\n",
    "            beam_nucs = lm.beam_propose(gpt_decoder.beam, context_window)\n",
    "            for c, (hyp, nextensions) in enumerate(gpt_decoder.get_hypotheses()):\n",
    "                nuc, logprobs = beam_nucs[c]\n",
    "                if len(nuc) < 1: continue\n",
    "                extend_words = [' '.join(hyp.words[-context_window:] + [x]) for x in nuc]\n",
    "                embs = model.encode(extend_words, convert_to_numpy=False, convert_to_tensor=True)\n",
    "                scores = tmf.pairwise_cosine_similarity(predicted_latents[[i]], embs)[0].cpu()\n",
    "                embs = [None] * len(embs)\n",
    "                local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, scores, embs)]\n",
    "                gpt_decoder.add_extensions(local_extensions, scores, nextensions)\n",
    "            gpt_decoder.extend(verbose = False)\n",
    "            context_window += 1\n",
    "            pbar.update(1)\n",
    "        best_hyp = np.argmax([sum(hyp.logprobs) for hyp in gpt_decoder.beam])\n",
    "        print(\"Correct chunk:\", row.text[i])\n",
    "        print(\"Best hypothesis:\", \" \".join(gpt_decoder.beam[best_hyp].words[-num_words:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, row = next(iter(df_train.iterrows()))\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "generated_chunks_lengths = []\n",
    "current_crop_length = 0\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(len(row.text))[:10]:\n",
    "    generated_sentences = gpt2(\n",
    "        prompt,\n",
    "        max_new_tokens=8,\n",
    "        num_return_sequences=1000,\n",
    "        pad_token_id=50256,\n",
    "        top_k=0,\n",
    "        top_p=0.6,\n",
    "        temperature=1.8,\n",
    "        repetition_penalty=1.8,\n",
    "    )\n",
    "    generated_sentences = [re.sub(r'\\n+', ' ', s[\"generated_text\"]) for s in generated_sentences]\n",
    "    if i > config[\"context_length\"]:\n",
    "        current_crop_length = generated_chunks_lengths[i - config[\"context_length\"] - 1]\n",
    "    generated_sentences_cropped = [s[current_crop_length:] for s in generated_sentences]\n",
    "    embeddings = model.encode(generated_sentences_cropped, convert_to_numpy=False, convert_to_tensor=True)\n",
    "    best_sentence_index = tmf.pairwise_cosine_similarity(predicted_latents[[i]], embeddings).argmax()\n",
    "    best_sentence = generated_sentences[best_sentence_index]\n",
    "    generated_chunks_lengths.append(len(best_sentence) - len(prompt))\n",
    "    print(\"Generated: \", best_sentence[len(prompt):])\n",
    "    print(\"Correct: \", row.text[i])\n",
    "    print(i, current_crop_length)\n",
    "    prompt = best_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# autoregressively generate summary (uses greedy decoding by default)\n",
    "generated_ids = model.generate(input_ids)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0, 3, 1, 2]).argsort()[::-1].argsort().argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encoder(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
