{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from src.main import main\n",
    "from src.prepare_latents import compute_chunks\n",
    "from src.utils import device, progress\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import clear_output\n",
    "import torchmetrics.functional as tmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"li2022_EN_SS_trimmed_mean\"]\n",
    "subjects = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"lebel2023\"]\n",
    "subjects = {\"lebel2023\": [\"UTS03\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"datasets\": datasets,\n",
    "    \"subjects\": subjects,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"decoder\": \"brain_decoder\",\n",
    "    \"loss\": \"mixco\",\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"context_length\": 6,\n",
    "    \"lag\": 3,\n",
    "    \"smooth\": 6,\n",
    "    \"stack\": 0,\n",
    "    \"dropout\": 0.7,\n",
    "    \"patience\": 20,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 1,\n",
    "    \"temperature\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = pipeline(\"text-generation\", model=\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = main(return_data=True, caching=False, **config)\n",
    "_, decoder = main(**config)\n",
    "decoder = decoder.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = []\n",
    "df = pd.concat([df_train[[\"dataset\", \"run\"]], df_valid[[\"dataset\", \"run\"]], df_test[[\"dataset\", \"run\"]]]).drop_duplicates()\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row.dataset == \"lebel2023\":\n",
    "        textgrid_path = f\"data/lebel2023/derivative/TextGrids/{row.run}.TextGrid\"\n",
    "    chunks = compute_chunks(textgrid_path, 2, 0)\n",
    "    num_words = [len(chunk.split(\" \")) for chunk in chunks]\n",
    "    df_chunks.append([row.dataset, row.run, chunks, num_words])\n",
    "df_chunks = pd.DataFrame(df_chunks, columns=[\"dataset\", \"run\", \"text\", \"num_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"text\"]).merge(df_chunks)\n",
    "df_valid = df_valid.drop(columns=[\"text\"]).merge(df_chunks)\n",
    "df_test = df_test.drop(columns=[\"text\"]).merge(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train[df_train.run == \"wheretheressmoke\"].iloc[0]\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Tang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_decoding.decoding.GPT import GPT\n",
    "from semantic_decoding.decoding.LanguageModel import LanguageModel\n",
    "from semantic_decoding.decoding.Decoder import Decoder, Hypothesis\n",
    "\n",
    "data_lm = Path(\"data/data_lm\")\n",
    "with open(data_lm / \"perceived\" / \"vocab.json\", \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(data_lm / \"decoder_vocab.json\", \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = data_lm / \"perceived\" / \"model\", vocab=gpt_vocab, device=device)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass=0.9, nuc_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_decoder = Decoder(word_times=range(sum(row.num_words)), beam_width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=sum(row.num_words)) as pbar:\n",
    "    for i, num_words in enumerate(row.num_words):\n",
    "        # if i > 0:\n",
    "        #     print(\"\\033[F\\033[F\", end='')\n",
    "        pbar.set_description(f\"Chunk {i+1} / {len(row.num_words)}\")\n",
    "        context_window = sum(row.num_words[max(0, i-config[\"context_length\"]):i])\n",
    "        for _ in range(num_words):\n",
    "            beam_nucs = lm.beam_propose(gpt_decoder.beam, context_window)\n",
    "            for c, (hyp, nextensions) in enumerate(gpt_decoder.get_hypotheses()):\n",
    "                nuc, logprobs = beam_nucs[c]\n",
    "                if len(nuc) < 1: continue\n",
    "                extend_words = [' '.join(hyp.words[-context_window:] + [x]) for x in nuc]\n",
    "                embs = model.encode(extend_words, convert_to_numpy=False, convert_to_tensor=True)\n",
    "                scores = tmf.pairwise_cosine_similarity(predicted_latents[[i]], embs)[0].cpu()\n",
    "                embs = [None] * len(embs)\n",
    "                local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, scores, embs)]\n",
    "                gpt_decoder.add_extensions(local_extensions, scores, nextensions)\n",
    "            gpt_decoder.extend(verbose = False)\n",
    "            context_window += 1\n",
    "            pbar.update(1)\n",
    "        best_hyp = np.argmax([sum(hyp.logprobs) for hyp in gpt_decoder.beam])\n",
    "        print(\"Correct chunk:\", row.text[i])\n",
    "        print(\"Best hypothesis:\", \" \".join(gpt_decoder.beam[best_hyp].words[-num_words:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(config[\"model\"], device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, row = next(iter(df_train.iterrows()))\n",
    "with torch.no_grad():\n",
    "    predicted_latents = decoder(decoder.projector[row.dataset + \"/\" + row.subject](row.X.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "generated_chunks_lengths = []\n",
    "current_crop_length = 0\n",
    "torch.cuda.empty_cache()\n",
    "for i in range(len(row.text))[:10]:\n",
    "    generated_sentences = gpt2(\n",
    "        prompt,\n",
    "        max_new_tokens=8,\n",
    "        num_return_sequences=1000,\n",
    "        pad_token_id=50256,\n",
    "        top_k=0,\n",
    "        top_p=0.6,\n",
    "        temperature=1.8,\n",
    "        repetition_penalty=1.8,\n",
    "    )\n",
    "    generated_sentences = [re.sub(r'\\n+', ' ', s[\"generated_text\"]) for s in generated_sentences]\n",
    "    if i > config[\"context_length\"]:\n",
    "        current_crop_length = generated_chunks_lengths[i - config[\"context_length\"] - 1]\n",
    "    generated_sentences_cropped = [s[current_crop_length:] for s in generated_sentences]\n",
    "    embeddings = model.encode(generated_sentences_cropped, convert_to_numpy=False, convert_to_tensor=True)\n",
    "    best_sentence_index = tmf.pairwise_cosine_similarity(predicted_latents[[i]], embeddings).argmax()\n",
    "    best_sentence = generated_sentences[best_sentence_index]\n",
    "    generated_chunks_lengths.append(len(best_sentence) - len(prompt))\n",
    "    print(\"Generated: \", best_sentence[len(prompt):])\n",
    "    print(\"Correct: \", row.text[i])\n",
    "    print(i, current_crop_length)\n",
    "    prompt = best_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
